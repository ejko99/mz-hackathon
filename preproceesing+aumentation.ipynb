{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train & validation data load\n",
    "data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sentence missing\n",
    "data = data.dropna(axis=0)\n",
    "val_data = val_data.dropna(axis=0)\n",
    "\n",
    "\n",
    "# Remove duplicate sentences\n",
    "data = data.drop_duplicates(['문장'], keep='first')\n",
    "val_data = val_data.drop_duplicates(['문장'], keep='first')\n",
    "\n",
    "\n",
    "# text col\n",
    "text = data['문장']\n",
    "val_text = val_data['문장']\n",
    "\n",
    "# label col\n",
    "tag = data['태그']\n",
    "val_tag = val_data['태그']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어절(띄어쓰기) 기준 tokenizing\n",
    "def tokenizing_text(texts):\n",
    "    corpus = []\n",
    "    for s in texts:\n",
    "        result = re.split(' ',str(s))\n",
    "        corpus.append(result)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizing_text(text)\n",
    "val_text = tokenizing_text(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in text:\n",
    "    if (s[0] == '줘') or (s[0] == '놔'):\n",
    "        s.pop(0)\n",
    "\n",
    "for s in text:\n",
    "    if s[-1] == '할.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in text:\n",
    "    if s[-1] == '등.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in text:\n",
    "    if s[-1] == '길.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in text:\n",
    "    if s[-1] == '길.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in text:\n",
    "    if s[-1] == '이.':\n",
    "        s[-1] = '줘.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same preprocess val_text\n",
    "\n",
    "for s in val_text:\n",
    "    if (s[0] == '줘') or (s[0] == '놔'):\n",
    "        s.pop(0)\n",
    "        \n",
    "for s in val_text:\n",
    "    if s[-1] == '할.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in val_text:\n",
    "    if s[-1] == '등.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in val_text:\n",
    "    if s[-1] == '길.':\n",
    "        s[-1] = '줘.'\n",
    "        \n",
    "for s in val_text:\n",
    "    if s[-1] == '주.':\n",
    "        s[-1] = '줘.'\n",
    "\n",
    "for s in val_text:\n",
    "    if s[-1] == '이.':\n",
    "        s[-1] = '줘.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence summation\n",
    "def str_sum(text):\n",
    "    temp = list()\n",
    "    for s in text:\n",
    "        temp.append(' '.join(s))\n",
    "    return temp\n",
    "\n",
    "text = str_sum(text)\n",
    "val_text = str_sum(val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug datafarme shuffle\n",
    "aug = aug.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "aug_text = list(aug['문장'])\n",
    "aug_tag = list(aug['태그'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def RandSwap(text):\n",
    "    temp = []\n",
    "    for s in text:\n",
    "        a = random.randint(0,len(s)-1)\n",
    "        b = random.randint(0,len(s)-1)\n",
    "        if a!=b:\n",
    "            s[a],s[b] = s[b],s[a]\n",
    "        temp.append(s)\n",
    "    return temp\n",
    "\n",
    "swap_text = tokenizing_text(aug_text)\n",
    "swap_text = RandSwap(swap_text)\n",
    "swap_text = str_sum(swap_text)\n",
    "aug1 = pd.DataFrame(swap_text,aug_tag)\n",
    "\n",
    "#swap_text[:10000]\n",
    "#aug_tag[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandDel(text):\n",
    "    temp = []\n",
    "    for s in text:\n",
    "        a = random.randint(0,len(s)-1)\n",
    "        s.pop(a)\n",
    "    temp.append(s)\n",
    "    return temp\n",
    "\n",
    "del_text = tokenizing_text(aug_text)\n",
    "del_text = RandSwap(del_text)\n",
    "del_text = str_sum(del_text)\n",
    "aug1 = pd.DataFrame(del_text,aug_tag)\n",
    "\n",
    "#del_text[:10000]\n",
    "#aug_tag[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aug_tag[:10000]\n",
    "\n",
    "idx_encode = preprocessing.LabelEncoder()  \n",
    "idx_encode.fit(aug_tag)\n",
    "Label_aug = idx_encode.transform(aug_tag) # 주어진 고유한 정수로 변환\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(del_text,aug_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = np.array(text)\n",
    "val_text = np.array(val_text)\n",
    "\n",
    "#tag = np.array(tag)\n",
    "val_tag = np.array(val_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data label encoding\n",
    "idx_encode = preprocessing.LabelEncoder()  \n",
    "idx_encode.fit(tag)\n",
    "Label_train = idx_encode.transform(tag) # 주어진 고유한 정수로 변환\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data label encoding\n",
    "idx_encode = preprocessing.LabelEncoder()  \n",
    "idx_encode.fit(val_tag)\n",
    "Label_test = idx_encode.transform(val_tag) # 주어진 고유한 정수로 변환\n",
    "\n",
    "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = pd.DataFrame(label_idx)\n",
    "raw = []\n",
    "encode = []\n",
    "for key in label_idx:\n",
    "    raw.append(key)\n",
    "#print(raw)\n",
    "#print(len(raw))\n",
    "for val in label_idx.values():\n",
    "    encode.append(val)\n",
    "decoder = pd.DataFrame({'label_num':encode, 'label_text':raw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.to_csv('decoder.txt', index=False)\n",
    "file = open('decoder.txt','r',encoding='utf-8')\n",
    "#file_open = file.read()\n",
    "#file.close()\n",
    "Decoder = pd.read_csv('decoder.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89622"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text + swap_text[:20000]\n",
    "text = text + del_text[20000:30000]\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_train  = list(Label_train)\n",
    "Label_aug = list(Label_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89622\n"
     ]
    }
   ],
   "source": [
    "Label_train = Label_train + Label_aug[:20000] + Label_aug[20000:30000]\n",
    "print(len(Label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(text)\n",
    "Label_train = np.array(Label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 순서 섞기\n",
    "\n",
    "s = np.arange(text.shape[0])\n",
    "np.random.shuffle(s)\n",
    "x_train = text[s]\n",
    "y_train = Label_train[s]\n",
    "\n",
    "'''\n",
    "s = np.arange(val_text.shape[0])\n",
    "np.random.shuffle(s)\n",
    "x_test = val_text[s]\n",
    "y_test = Label_test[s]\n",
    "'''\n",
    "x_test = val_text\n",
    "y_test = Label_test\n",
    "# dev.txt를 validation set으로 사용 -----> split 함수 사용하지 X \n",
    "#x_train, x_test = train_test_split(TextData, test_size=0.2, shuffle=False)\n",
    "#y_train, y_test = train_test_split(LabelData, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in .TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write train data to train.tsv ...\n",
      "Write test data to test.tsv ...\n"
     ]
    }
   ],
   "source": [
    "#write in tsv\n",
    "\n",
    "with open('0105_train.tsv', 'wt', newline='', encoding='utf-8-sig') as f:\n",
    "    print('Write train data to {} ...'.format('train.tsv'))\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(zip(x_train, y_train))\n",
    "with open('0105_test.tsv', 'w', newline='', encoding='utf-8-sig') as f:\n",
    "    print('Write test data to {} ...'.format('test.tsv'))\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(zip(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
