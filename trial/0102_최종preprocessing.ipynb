{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "0102_최종preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESMvN8R6_daJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS8DHNtR_daW"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KlLZOYtF_daX"
      },
      "source": [
        "# train & validation data load\n",
        "data = pd.read_csv('train.csv')\n",
        "val_data = pd.read_csv('dev.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-nU8kLP_daY"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XrcBqsx_daZ"
      },
      "source": [
        "# Remove sentence missing\n",
        "data = data.dropna(axis=0)\n",
        "val_data = val_data.dropna(axis=0)\n",
        "\n",
        "\n",
        "# Remove duplicate sentences\n",
        "data = data.drop_duplicates(['문장'], keep='first')\n",
        "val_data = val_data.drop_duplicates(['문장'], keep='first')\n",
        "\n",
        "\n",
        "# text col\n",
        "text = data['문장']\n",
        "val_text = val_data['문장']\n",
        "\n",
        "# label col\n",
        "tag = data['태그']\n",
        "val_tag = val_data['태그']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k14NqcD_daZ"
      },
      "source": [
        "# 어절(띄어쓰기) 기준 tokenizing\n",
        "def tokenizing_text(texts):\n",
        "    corpus = []\n",
        "    for s in texts:\n",
        "        result = re.split(' ',str(s))\n",
        "        corpus.append(result)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QViYefSO_daa"
      },
      "source": [
        "text = tokenizing_text(text)\n",
        "val_text = tokenizing_text(val_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMe7eUTD_dac"
      },
      "source": [
        "for s in text:\n",
        "    if (s[0] == '줘') or (s[0] == '놔'):\n",
        "        s.pop(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjgQiWn0_dad"
      },
      "source": [
        "for s in text:\n",
        "    if s[-1] == '할.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug7kZVlA_daf"
      },
      "source": [
        "for s in text:\n",
        "    if s[-1] == '등.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE5fCqhx_dag"
      },
      "source": [
        "for s in text:\n",
        "    if s[-1] == '길.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXk8diVH_dah"
      },
      "source": [
        "for s in text:\n",
        "    if s[-1] == '주.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsdxuZ8v_daj"
      },
      "source": [
        "for s in text:\n",
        "    if s[-1] == '이.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-G42XuY_daj"
      },
      "source": [
        "# same preprocess val_text\n",
        "\n",
        "for s in val_text:\n",
        "    if (s[0] == '줘') or (s[0] == '놔'):\n",
        "        s.pop(0)\n",
        "        \n",
        "for s in val_text:\n",
        "    if s[-1] == '할.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)\n",
        "        \n",
        "for s in val_text:\n",
        "    if s[-1] == '등.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)\n",
        "        \n",
        "for s in val_text:\n",
        "    if s[-1] == '길.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)\n",
        "        \n",
        "for s in val_text:\n",
        "    if s[-1] == '주.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)\n",
        "\n",
        "for s in val_text:\n",
        "    if s[-1] == '이.':\n",
        "        s[-1] = '줘.'\n",
        "        print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68h0oeer_dak"
      },
      "source": [
        "# sentence summation\n",
        "def str_sum(text):\n",
        "    temp = list()\n",
        "    for s in text:\n",
        "        temp.append(' '.join(s))\n",
        "    return temp\n",
        "\n",
        "text = str_sum(text)\n",
        "val_text = str_sum(val_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYhPNfC0_dal"
      },
      "source": [
        "## Label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2sNRfd1_dam"
      },
      "source": [
        "text = np.array(text)\n",
        "val_text = np.array(val_text)\n",
        "\n",
        "tag = np.array(tag)\n",
        "val_tag = np.array(val_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIo8XN_O_dam"
      },
      "source": [
        "# train data label encoding\n",
        "idx_encode = preprocessing.LabelEncoder()  \n",
        "idx_encode.fit(tag)\n",
        "Label_train = idx_encode.transform(tag) # 주어진 고유한 정수로 변환\n",
        "\n",
        "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
        "#print(label_idx)\n",
        "#print(Label_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn7NBHZY_dam"
      },
      "source": [
        "# validation data label encoding\n",
        "idx_encode = preprocessing.LabelEncoder()  \n",
        "idx_encode.fit(val_tag)\n",
        "Label_test = idx_encode.transform(val_tag) # 주어진 고유한 정수로 변환\n",
        "\n",
        "label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
        "#print(Label_test)\n",
        "#print(type(Label_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D5Lsfb9_dan"
      },
      "source": [
        "## Data shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRN0mZbT_dan"
      },
      "source": [
        "# 데이터 순서 섞기\n",
        "\n",
        "s = np.arange(text.shape[0])\n",
        "np.random.shuffle(s)\n",
        "x_train = text[s]\n",
        "y_train = Label_train[s]\n",
        "\n",
        "\n",
        "s = np.arange(val_text.shape[0])\n",
        "np.random.shuffle(s)\n",
        "x_test = val_text[s]\n",
        "y_test = Label_test[s]\n",
        "\n",
        "\n",
        "# dev.txt를 validation set으로 사용 -----> split 함수 사용하지 X \n",
        "#x_train, x_test = train_test_split(TextData, test_size=0.2, shuffle=False)\n",
        "#y_train, y_test = train_test_split(LabelData, test_size=0.2, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jH5CdSa_dao",
        "outputId": "86b1e37c-217f-4aae-dc10-0f9ebb14986b"
      },
      "source": [
        "print(\"text \", type(text))\n",
        "print(\"val_text \", type(val_text))\n",
        "print(\"tag \", type(tag))\n",
        "print(\"val_tag \", type(val_tag))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text  <class 'numpy.ndarray'>\n",
            "val_text  <class 'numpy.ndarray'>\n",
            "tag  <class 'numpy.ndarray'>\n",
            "val_tag  <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_kRYFo_dao"
      },
      "source": [
        "## Save in .TSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJvvZ2-v_dao",
        "outputId": "21c1835a-0823-4e86-8b8c-a4282cb84c4e"
      },
      "source": [
        "#write in tsv\n",
        "\n",
        "with open('전처리train.tsv', 'wt', newline='', encoding='utf-8-sig') as f:\n",
        "    print('Write train data to {} ...'.format('train.tsv'))\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(x_train, y_train))\n",
        "with open('전처리test.tsv', 'w', newline='', encoding='utf-8-sig') as f:\n",
        "    print('Write test data to {} ...'.format('test.tsv'))\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "    writer.writerows(zip(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Write train data to train.tsv ...\n",
            "Write test data to test.tsv ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr0fN4D_dap"
      },
      "source": [
        "# 주석.........."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Dx65Mtru_dap",
        "outputId": "c9777832-1216-47a3-e9cb-1ead3fa35fbf"
      },
      "source": [
        "'''\n",
        "#### define preprocessing func ####\n",
        "\n",
        "# 특수문자 제거\n",
        "def clean_text(texts):\n",
        "    corpus = []\n",
        "    for s in texts:\n",
        "        result = re.sub('[^ ㄱ-ㅣ가-힣]+','', s) \n",
        "        corpus.append(result)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "# 어절(띄어쓰기) 기준 tokenizing\n",
        "def tokenizing_text(texts):\n",
        "    corpus = []\n",
        "    for s in texts:\n",
        "        result = re.split(' ',s)\n",
        "        corpus.append(result)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "\n",
        "## pos tagging (형태소 품사) 기준 tokenizing\n",
        "from konlpy.tag import Kkma \n",
        "kkma = Kkma()\n",
        "\n",
        "def pos_tokenizing(texts):\n",
        "    corpus = []\n",
        "    for s in texts:\n",
        "        result = kkma.morphs(s)\n",
        "        corpus.append(result)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "\n",
        "## stopwords\n",
        "stop_words = \"하 어 가 아\"    # 불용어 지정\n",
        "stop_words = stop_words.split(' ')\n",
        "\n",
        "def stopwords(texts):\n",
        "    temp1 = []\n",
        "    for sentence in texts:\n",
        "        temp2 = []\n",
        "        for words in sentence:\n",
        "            if words not in sotp_words:\n",
        "                temp2.append(words)\n",
        "            temp1.append(temp2)\n",
        "    return temp1\n",
        "\n",
        "# 중복 문장 확인\n",
        "data['문장'].value_counts()\n",
        "\n",
        "# 문장 다시 합치기\n",
        "def str_sum(text):\n",
        "    temp = list()\n",
        "    for s in text:\n",
        "        temp.append(' '.join(s))\n",
        "    return temp\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#### define preprocessing func ####\\n\\n# 특수문자 제거\\ndef clean_text(texts):\\n    corpus = []\\n    for s in texts:\\n        result = re.sub(\\'[^ ㄱ-ㅣ가-힣]+\\',\\'\\', s) \\n        corpus.append(result)\\n    return corpus\\n\\n\\n# 어절(띄어쓰기) 기준 tokenizing\\ndef tokenizing_text(texts):\\n    corpus = []\\n    for s in texts:\\n        result = re.split(\\' \\',s)\\n        corpus.append(result)\\n    return corpus\\n\\n\\n\\n## pos tagging (형태소 품사) 기준 tokenizing\\nfrom konlpy.tag import Kkma \\nkkma = Kkma()\\n\\ndef pos_tokenizing(texts):\\n    corpus = []\\n    for s in texts:\\n        result = kkma.morphs(s)\\n        corpus.append(result)\\n    return corpus\\n\\n\\n\\n## stopwords\\nstop_words = \"하 어 가 아\"    # 불용어 지정\\nstop_words = stop_words.split(\\' \\')\\n\\ndef stopwords(texts):\\n    temp1 = []\\n    for sentence in texts:\\n        temp2 = []\\n        for words in sentence:\\n            if words not in sotp_words:\\n                temp2.append(words)\\n            temp1.append(temp2)\\n    return temp1\\n\\n# 중복 문장 확인\\ndata[\\'문장\\'].value_counts()\\n\\n# 문장 다시 합치기\\ndef str_sum(text):\\n    temp = list()\\n    for s in text:\\n        temp.append(\\' \\'.join(s))\\n    return temp\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}